# ARoW
This repository contains the code for ICML 2023 paper "Improving adversarial robustness by putting more regularizations on less robust samples" by Dongyoon Yang, Insung Kong and Yongdai Kim.

# Train

`code`

# Evaluation

The trained models can be evaluated by running eval.py which contains the standard accuracy and robust accuracies against PGD and AutoAttack.

`code`

# Citation


`
@inproceedings{
    dongyoon2023improving,
    title={Improving adversarial robustness by putting more regularizations on less robust samples},
    author={Dongyoon Yang, Insung Kong and Yongdai Kim},
    booktitle={International Conference on Machine Learning},
    year={2023},
    url={https://arxiv.org/abs/2206.03353}
}
`
